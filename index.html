<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Alex M. Ascensión - Web</title>

	<link rel="stylesheet" href="css/reset.css">
	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/mytheme.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/monokai.css">

	<!-- For custom controls -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement( 'link' );
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>
</head>
<body>
	<div class="reveal">
		<div class="slides">
			<section id="titulo-y-paper">
				<section style="width: 100%">
					<h2 style="text-align: left; color: #8c0d04">
						Common stats mistakes from (and for) scientists
					</h2>
					<hr>
					<table style="width:100%; padding-left: 0%">
						<tr style="color: #909090; ">
							<th style="font-weight: lighter; padding-left: 0%; padding-top: 10%">
								Alex M. Ascensión
							</th>
							<th style="font-weight: lighter; padding-right: 0%; padding-top: 10%; text-align: right">
								<span id="datetime"></span>

								<script>
									var dt = new Date();
									document.getElementById("datetime").innerHTML = dt.toLocaleDateString();
								</script>
							</th>
						</tr>
					</table>

					<aside class="notes">
						This is a presentation I've been thinking of for a long time. I won't be explaining my results, as in the typical wetlab presentation, but I will focus on common mistakes scientists, and non-scientists, do when doing the analysis of their experiments.
					</aside>
				</section>

				<section id="portada imagen">
					<img class="plain" src="figs/portada.png">

					<aside class="notes">
						The main source of inspiration is this paper. In this paper they explain ten common statistical mistakes often done in academia, and which should be accounted for to improve the analysis. In this presentation I will inlude most of them, so it might be a bit lengthy.
					</aside>
				</section>

				<section id="portada imagen">
					<iframe width="80%" height="600" src="https://www.youtube.com/embed/Hz1fyhVOjr4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

					<aside class="notes">
						To begin with, I would like to show you a video that Olga showed me a while ago. It might be a satirical representation of the interaction between biologists and statisticians, but this kind of situation does indeed commonly happen. // In this presentation I will guide towards the mathematical or logical mistakes that we usually make when doing data analysis or performing an experiment. BUT, as we will see later, other SOCIAL factors are relevant for good science.
					</aside>
				</section>
			</section>
			

			<section>
				<section data-background="#94346E">
					<h1>.</h1>
					<h2>101 STATS</h2>
					<aside class="notes">
						Just before we start, I want to make sure that some basics are known. I guess that you all remember the statistical tests and all this stuff, but I will do some refreshing, because great part of the presentation will be based on that, as you can imagine.
					</aside>
				</section>

				<section>
					<h2>Descriptors: median (M) and Median Absolute Deviation (MAD)</h2>
					$\small{\text{Mean: } \mu = \frac{ 1}{N}\sum_i^N x_i \qquad \text{Variance: } \sigma^2 =  \frac{1}{N}\sum_i^N(x_i - \mu)^2}$
					<img src="figs/median-MAD.png" class='plain' style="width: 35%">
					<aside class="notes">
						Before I talk about tests, I want to make sure we know about some statistical descriptors. We know what mean and standard deviation are. The idea of the mean is to find the "balance" point of the distribution, and the std tries to see how far from the mean the data lays. There are two other commonly used descriptors, equivalent to the mean and std: the median, and the MAD (we will say why they are important later). ** In the MAD, we SUBSTRACT THE MEDIAN FROM THE SAMPLES, as similar to the std** We will explain why they are important later, but simpy put, those descriptors are more robust in the sense that if I replace one of the numbers for other (6 for 10, 1 for 0, etc.) the descriptors should not change that much.
					</aside>
				</section>

				<section>
					<h2>Distributions</h2>
					<table align="center" valing="center" style="padding: 0">
						<tr>
							<td  align="center" style="padding: 0; vertical-align: middle" >
								<img src="figs/H0H1.svg" class='plain' style="margin: 1">
							</td>
							<td align="center" style="padding: 0; vertical-align: middle">
								<img src="figs/H0H1-table.svg" class='plain' style="height: 70%">
							</td>
						</tr>

					</table>

					<aside class="notes">
						In this slides I will talk abouit two SIMILAR BUT SEPARATE THINGs. First, I want you to recall the famous alpha value, what is it, and also want you to understand the beta and 1-beta values. **Explain the two distributions about diabetes -> from the explain FP / FN etc -> from there explain alpha and beta**. RECALL THAT 1-beta WILL BE IMPORTANT IN THE REST OF PRESENTATION. As for hypotesis testing, *explain how contrasts are formulated* 
					</aside>
				</section>

				<section>
					<h2>Inference tests</h2>
					<p style="text-align: center; %">$\begin{cases}
						\scriptsize{ H_0: \hat{\mu}_{H_1} \le / = \mu_{H_0} }\\
						\scriptsize{ H_1: \hat{\mu}_{H_1} >/\neq \mu_{H_0} }
					\end{cases} \rightarrow t, F, \chi^2,\cdots \rightarrow p\text{ - value}$</p>

					<img src="figs/H0H1.svg" class='plain' width="40%" style="margin-bottom: 4%">
					<img src="figs/p-value.svg" width="50%" class="plain" style="margin-left: 2%">
					<aside class="notes">
						The second part is the iference test. With the example above, for instance we want to test whether the set of diabetics and the non-diabetics have different glucose levels. In order to do that, we have to transform this "notion" of difference into a number. This number is called stadistic. The fun thing about statistics is that THEY HAVE BEEN CRAFTED TO FOLLOW A CERTAIN DISTRIBUTION. So, why does the statistic follow that distribution? Suppose we take 10 random numbers from H0, and other 10 random numbers from H0, then compute the function for t. Now take other 10 points and other 10 points, it will give another point. The "sum" of all possible combinations of 10 numbers and 10 numbers produces a distribution, which is the distribution of the statistic. THUS, if we take 10 points from H0 and, NOW, 10 points from H1, where does the t value lay on? The more different the points, the more to the right or the left of the distribution they will go. So, if a point is far right, it can be because we have chosen two sets of points from H0 that are very different, or a set of H0 and a set of H1. Which is more likely? That's what we set the alpha for.  // SO now that we have grasped the basics of stats, we'll move on to see how can we fail using statistics!
					</aside>
				</section>



			</section>

			<section>
				<section data-background="#CC503E">
					<h1>I</h1>
					<h2>Absence of [quality] <br> control group</h2>
				</section>

				<section style="text-align: left">
					<aside class="notes">
						What do we want to say with QC? Let's put an example. Say we want to study the effect of a drug on schizofrenic patients. // 

						Then we will have a experimental group with certain variables like weight // height // age // and other variables. Among them, the fact that they are schizofrenic and that they are taking the drug. // 

						In an ideal control, we would like a set of individuals that have the same values for all the variables but one: the drug. In this case, they should be taking a placebo. // 

						BUT, the common thing is that, when designing an experiment, the variables don't have the same values. Indeed, there are a set of common problems regarding controls that are not usually taken into account: // ->
					</aside>
					<!-- <h3 style="padding-bottom: 5%">The problem</h3> -->
					<div>
						<p> <span class="fragment">Test</span></p>
						<p>
							<span class="fragment">$\stackrel{\text{weight}}{\normalsize  A}\;$</span>
							<span class="fragment">$\stackrel{\text{height}}{\normalsize B}\;$</span>
							<span class="fragment">$\stackrel{\text{age}}{\normalsize C}\;$</span>
							<span class="fragment"> $\cdots\;$
								$\stackrel{\text{income}}{\normalsize X}\;$
								$\stackrel{\text{schizophrenia}}{\normalsize Y}\;$
							$\stackrel{\text{drug X}}{\normalsize Z}$ </span>
						</p>

						<span class="fragment">
							<p style="padding-top: 1%"> Control [ideal]</p>
							<p>
								$\stackrel{\text{weight}}{\normalsize A}\;$
								$\stackrel{\text{height}}{\normalsize B}\;$
								$\stackrel{\text{age}}{\normalsize C}\;$
								$\cdots\;$
								$\stackrel{\text{income}}{\normalsize X}\;$
								$\stackrel{\text{schizophrenia}}{\normalsize Y}\;$
								$\stackrel{\text{placebo}}{\normalsize \color{#8c0d04}{Z^*}}$ 
							</p>
						</span>

						<span class="fragment">
							<p style="padding-top: 1%"> Control [real]</p>
							<p>
								$\stackrel{\text{weight}}{\normalsize A}\;$
								$\stackrel{\text{height}}{\normalsize \color{#8c0d04}{B^*}}\;$
								$\stackrel{\text{age}}{\normalsize C}\;$
								$\cdots\;$
								$\stackrel{\text{income}}{\normalsize \color{#8c0d04}{X^*}}\;$
								$\stackrel{\text{schizophrenia}}{\normalsize Y}\;$
								$\stackrel{\text{placebo}}{\normalsize \color{#8c0d04}{Z^*}}$ 
							</p>
						</span>
					</div>
				</section>

				<section style="text-align: left">
					<aside class="notes">
						In social experiments, sometimes experimenters are not blinded (they explain the aim of the study), and this impacts the results // 

						Sometimes control and experiment groups are sampled at different times, and time should be considered another variable on the experiment // 

						Sometimes the number of samples in the control set and the experimental set are not the same. I will not talk about this on the presentation, but this is really
						common on Big Data analysis. For instance, we want to predict whether a breast image contains a cancer or not. In this case, it is common that there are many more images without cancer, and this usually is a problem for the algorithms. // 

						An the most important one: many times all variables are not considered, and this can bias the experiment for a certain case. I will show you two examples on that: // ->
					</aside>
					<h2>Common problems</h2>
					<ul>
						<span class="fragment"><li>Experimenters are not blinded</li></span>
						<span class="fragment"><li>Control & exp. groups sampled at different times</li></span>
						<span class="fragment"><li>$N_{\text{control}} \neq N_{\text{exp}}$</li></span>
						<span class="fragment"><li>All biological <b>heterogeneity</b> is <b>not considered</b> in the experimental / control group.</li></span>

					</ul>
				</section>

				<section style="text-align: left" data-background="figs/dummy.jpg" data-background-opacity=0.2>
					<aside class="notes">
						All produced cars require a security test. Usually this tests consist of a dummy inside the car, and the car crashes. Based on the results we can infer the security of the car.
						During decades, the dummies have been designed to consider an average male, and therefore some features of the cars, like the position of security belt, or airbags, has been adjusted for that dummy. As a result...  CITE
					</aside>

					<h3>Some examples (I)</h3>
					<p>Dummies are designed as average <b>male</b> (based on height/weight) </p>
					<blockquote>
						&ldquo;The fatality risk of a female driver is an estimated 17.0 ± 1.5 percent higher than for a male of the same age, given similar physical insults.&rdquo;
						<a href="https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/811766"> NHTSA </a>
					</blockquote> 
				</section>

				<section style="text-align: left" data-background="figs/ambien.jpg" data-background-opacity=0.15>
					<aside class="notes">
						The second case is of a drug, Ambien. Ambien is a sedative // When the drug was approved, the clinical trials featured "typical" caucasic men. // However, women metabolize ambien more slowly, so the drug does its effect more time. Many women reported that they still felt groggy while driving early in the morning, and in some cases traffic accidents happened // As a result, FDA recommended halving the intake of Ambien for women.
					</aside>

					<h3>Some examples (II)</h3>
					<span class="fragment"><p>Ambien (zolpidem) is a sedative.</p></span> 
					<span class="fragment"><p>Clinical trials were done in caucasic men, age 30, weight $\sim$ 70 kg.</p></span> 
					<span class="fragment"><p>Women metabolize ambien more slowly $\rightarrow$ women driving to work still groggy.</p></span> 
					<span class="fragment"><p>FDA recommended halving the intake (<a href="https://www.nytimes.com/2013/01/11/health/fda-requires-cuts-to-dosages-of-ambien-and-other-sleep-drugs.html">source</a>).</p></span> 
				</section>

				<section style="text-align: left">
					<h2>Solution <span class="fragment" style="color: #CC503E">[hard]</span></h2>
					<span class="fragment"><p>Be aware of as many variables as possible</p>
					</span>
					<span class="fragment"><p>Present conclusions as tentative</p></span>

					<aside class="notes">
						So, what can we do to minimize this error? // First, try to minimize the possible sources of variation, or account for them. This requires READING MORE BIBLIOGRAPHY ON PREVIOUS TRIALS OR SIDE EFFECTS THAT HAVE BEEN FOUND. // However, this many times is like finding a needle in a haystack, so we should present conclusions as tentative.
					</aside>
				</section>
			</section>



			<section>
				<section data-background="#EDAD08">

					<h1>II</h1>
					<h2>Inflating the degrees <br> of freedom</h2>
				</section>

				<section style="text-align: left">
					<aside class="notes">
						Another common statistic concept are the degrees of freedom // READ DESCRIPTION // How can intuitively grasp N-1. Lets imagine we have a population of 10 people, and we know their mean. Then, we don't need one of the elements of the population, because it can be recovered using the mean and the rest of the population. However, if we have 8 people, we cannot recover that information, because we lack 2 elements. Thus, we say that the DoF are N-1 // Same with two means, each from one population.
					</aside>
					<h2>What is a degree of freedom?</h2>
					<span class="fragment"><p>N of <b>independent</b> pieces of information to calculate the estimate</p>
					</span>
					<ul>
						<span class="fragment"><li>DoF 1 sample: $N-1$</li></span>
						<span class="fragment"><li>DoF 2 samples: $N_1 + N_2 - 2$</li></span>
					</ul>
				</section>	

				<section style="text-align: left">
					<aside class="notes">
						So, why is important to know the degrees of freedom of our sample? // I am going to try to convince you. For that, we will start easy, with 1 population. We are going to simulate some random points taken from a Normal distribution with mean 0.5 and deviation of 1. // We want to test if the mean of the population is greater than zero, so we do a test. // ->
					</aside>

					<h2>Why are DoF important?  </h2><h4>(1 sample)</h4>
					<span class="fragment">
						We are going to simulate normal data: $X \sim N(\mu_X, \sigma_X)$.
						We want to test whether $\mu_X > 0$. </span>
						<span class="fragment"> <p style="text-align: center; padding-top: 5%">$\begin{cases}
							H_0: \hat{\mu}_X \le 0 \\
							H_1: \hat{\mu}_X > 0

						\end{cases}$</p></span>

					</section>	

					<section >
						<aside class="notes">
							Let's suppose two cases: on the left, we just take 20 points from that distribution, and we do the test. The DoF are 19, so with that number of samples the p-val is 0.11 and we cannot assess that, with those 20 points, the distribution they come from has a mean greater than 0. // 
							
							BUT let's imagine that, for some reason, we duplicate the DoF. To do the calculations, this is the equivalent as "duplicating" the points from the previous dataset. In that case, 
							the number of points is 40, the DoF 39 and the p-value is 0.02, so we wan assess that the distribution these data come from has a mean greater than 0.
						</aside>
						<h2 style="text-align: left">Why are DoF important?  </h2><h4 style="text-align: left">(1 sample)</h4>
						<img src="figs/II-pvalues-20VS40.svg" text-align="center" class="plain" width="65%">						
					</section>	

					<section >
						<aside class="notes">
							So, we have seen the case for N = 20. And what about the rest? Well, if we use the simulation from before, but extend it to up to 100 samples, we see that until 30 data points, the p-value is greater than 0.05. But if out N lays between 15 and 30, mistaking the DoF can make our data be statistically significant. If we make harsher conditions (N(1,1)), then N is smaller, and there is not some much risk, but if the number of data is small (WE ARE USING 4 MICE FOR INSTANCE), the same as before might happen.
						</aside>
						<h2 style="text-align: left">Why are DoF important?  </h2><h4 style="text-align: left">(1 sample)</h4>
						<img src="figs/II-pvalues-N.svg" text-align="center" class="plain" width="65%">								
					</section>	

					<section >
						<aside class="notes">
							So, now that we understand the problem, let's focus on a more "real" case. // Let's supose we have 20 patients with schizophrenia, 10 of which receive a placebo, and 10 of which receive a treatment. We want to test whether the drug has a positive impact on the illness. // Then, what is the DoF? // Since we are running a hypothesis with two groups with different people on each, the DoF are 18.
						</aside>
						<h2 style="text-align: left">Extending DoF to 2 samples.</h2>
						<span class="fragment">
							<img src="figs/II-DROGA+PLACEBO.svg" text-align="center" class="plain" width="100%">	
						</span>


						<div class="row">
							<div class="column"><span class="fragment"><p>What is the DoF?</p></span></div>
							<div class="column"><span class="fragment"><p>20 - 2 = 18</p>
								<p>$\tiny{\begin{cases}
									H_0: \mu_A = \mu_B\\
									H_1: \mu_A \neq \mu_B
								\end{cases}}$</p></span>
							</div>
						</div> 
					</section>	


					<section >
						<aside class="notes">
							Now, lets suppose that we have another clincal trial with 10 patients. We measure some variable, give them the drug, and then measure the same variable after some time. // Now, what is the DoF? // In this case, although we have 20 measurements we don't have 20 people, but 10. Indeed, the way to test the relevance of the trial is to compute differente before and after the experiment. Thus, the DoF is 9.
						</aside>
						<h2 style="text-align: left">Extending DoF to 2 samples.</h2>
						<span class="fragment">
							<img src="figs/II-DROGA+TIEMPO.svg" text-align="center" class="plain" width="100%">	
						</span>


						<div class="row">
							<div class="column"><span class="fragment"><p>What is the DoF?</p></span></div>
							<div class="column"><span class="fragment"><p>10 - 1 = 9</p>
								<p>$\tiny{D = x_t - x_0\;\;\begin{cases}
									H_0: \mu_D = 0\\
									H_1: \mu_D \neq 0
								\end{cases}}$</p></span>
							</div>
						</div> 
					</section>	


					<section style="text-align: left">
						<aside class="notes">
							So, if we have run our test incorrectly, what can we do? // Run the correct test and adapt the number of samples. // And if the results now turn out negative present them as tentative.
						</aside>
						<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
						<span class="fragment"><p>Adapt to DoF and rerun the test.</p></span>
						<span class="fragment"><p>If results become not significant, present them as tentative.</p></span>
					</section>

				</section>

				<section>
					<section data-background="#E17C05">
						<aside class="notes">
							
						</aside>
						<h1>III</h1>
						<h2>Interpret <div style="color: #FFFFFF">comparisons</div> between groups <div style="color: #FFFFFF">without comparing</div> them</h2>
					</section>

					<section>
						<aside class="notes">
							It is common to have several experiments done at once. Sometimes results from two experiments might be different, so we automatically assume that the datasets by themselves are different, and this might not be the case. In this example we did two correlations, one for each color. We see that for the red color the correlation was non-significant, and for the blue one it was significant.
							HOWEVER, if we run a Kolmogorov-Smirnov test, which is a test to compare two distributions, it returns a negative p-value, so we cannot ascertain that the distributions from which the two sets come are different.
						</aside>
						<img src="figs/III-corrs.svg"  class="plain" style="width: 90%; margin-left: -8%">
					</section>
					<section>
						<aside class="notes">
							In this case, we have two sets of points that come from the same distribution N(2,2). We want to test whether the mean of the distribution is 0. For the red case, it is, but for the blue one it is not. However a t-test of two independent samples says that both distributions are different.
						</aside>
						<img src="figs/III-ttest.svg"  class="plain" style="width: 88%">
						<p style="text-align: center; margin-top: -2%">$\tiny{\begin{cases}
							H_0: \mu_X \le 0 \\
							H_1: \mu_X > 0
						\end{cases}}$</p>
					</section>
					<section>
						<aside class="notes">
							And we can pursue just the opposite. In this case the two distributions are different N(1, 2), N(0.5, 2), as we see with the t-test of two samples, but each individual test to see if the mean is different than zero yields negative.
						</aside>
						<img src="figs/III-ttest-diff.svg"  class="plain" style="width: 88%">
					</section>
					<section style="text-align: left">
						<aside class="notes">
							So, what's the solution? Well, if you want to publish the indirect comparisons, run the tests!
						</aside>
						<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
						<span class="fragment"><p>Avoid comparing statistics from different tests at once</p>
						</span>
						<span class="fragment"><p>Run tests to compare the two effects (e.g. ANOVA)</p></span>
					</section>
				</section>



				<section>
					<section data-background="#73AF48">
						<h1>IV</h1>
						<h2>Data is not normal <br> (and you should not rely only on estimators)</h2>
						<aside class="notes">
							The classical statistics as we know is usually based on the normal distribution. The normal distribution has a certain set of mathematical properties that makes it "beautiful". 
							But, as usual ...// 
						</aside>
					</section>

					<section style="text-align: left">
						<aside class="notes">
							... there are many cases, in biology or any other realm of knowledge, were variables do not follow a normal distribution. In out case [the computationals (TM)], gene expression data, whether in bulk or in single cell, is never normal. In this paper they purge over 1000 papers and found many of them with nonnormal distributions. More interestingly, this NUMBER IS EVEN HIGHER, because many papers disguise their experimental distributions as normal when, in reality, they are not.
						</aside>
						<h3>Many biological variables are not normal!</h3>
						<img src="figs/portada-nonnormal.png" class='plain' style="margin-top: -2%; margin-bottom: -2%">
						<p>231 curated papers with non-normal data: Gamma, Negative Binomial, Binomial, etc.</p>
					</section>


					<section style="text-align: left">
						<aside class="notes">
							To see the relevance of "classical estimators", we see that with the distributions that are not normal, and which are skewed to the right, the mean usually appears more to the right on those distributions. However, the mean should represent the "center of mass" of the distribution, and in that case it is more logical to find the mean nearer to the point of the distribution with more height. In that sense, the median is nearer to that "expected" point than the median. 
						</aside>
						<h3>Robust estimators for non-normal distributions</h3>
						<img src="figs/IV-mean-M-dists.svg" class="plain">
					</section>

					<section style="text-align: left">
						<aside class="notes">
							Also, if we consider outliers, M and MAD are more reobust. In this image, we set one point, in red, as the moving point of the distribution, that is, the outlier. Blue and violet vertical lines represent the mean and median of the distributions, and horizontal lines represent variance and MAD. We see that, in the presence of the outlier, M and MAD don't change, whereas the mean and std change drastically.
						</aside>
						<h3>Robust estimators are better with outliers</h3>
						<img src="figs/IV-meanMAD-scatter.svg" class="plain">
					</section>



					<section style="text-align: left" data-transition="none-out">
						<aside class="notes">
							Sometimes relying only on descriptors might be a problem. // Here I show you a dataset of 4 points that share statistical descriptors. You see those points... 
						</aside>
						<h3>Anscombe's quartet: why using descriptors is not enough</h3>
						<p>Datasets from different distributions can share similar descriptors!</p>
						<span class="fragment"><img src="figs/anscombe.png" class='plain' ></span>
					</section>

					<section style="text-align: left" data-transition="none-in">
						<h3>Anscombe's quartet: why using descriptors is not enough</h3>
						<aside class="notes">
							... and you see here the descriptors. They share mean, std and corr in X and Y variables. They don't share median or MAD, mainly because the datset has been crafted to be in this way, but it could be easily crafted to do so!
						</aside>
						<p>Datasets from different distributions can share similar descriptors!</p>
						<table style="width:35%; font-size: 25px">
							<tr><td><span style="color: #73AF48">$\mu_X$: 9</span></td></tr>
							<tr><td><span style="color: #73AF48">$\sigma_X$: 11</span></td></tr>
							<tr><td><span style="color: #73AF48">$\mu_Y$: 7.5</span></td></tr>
							<tr><td><span style="color: #73AF48">$\sigma_Y$: 4.125</span></td></tr>
							<tr><td><span style="color: #73AF48">Correlation $X,Y$: 0.816</span></td></tr>
							<tr><td><span style="color: #73AF48">Linear regression: $y = 3 + 0.5x$</span></td></tr>
							<tr><td><span style="color: #CC503E">Median</span></td></tr>
							<tr><td><span style="color: #CC503E">Median Absolute Deviation</span></td></tr>
						</table> 
					</section>

					<section style="text-align: left">
						<h2>Datasaurus!</h2>
						<aside class="notes">
							It gets even more interesting with this dataset. This dataset was crafted by two guys in autodesk Justin Matejka and George Fitzmaurice. They based the rest of the datasets on the T-rex dataset using an algorithm.
						</aside>
						<span class="fragment"><img src="figs/datasaurus.png" class='plain' style="width: 75%; text-align: center">
							<a href="https://www.autodeskresearch.com/sites/default/files/SameStats-DifferentGraphs.pdf" style="font-size: 0.6em"> Source </a></span>

						</section>


						<section style="text-align: left">
							<aside class="notes">
								// So... having seen that mean and std don't always serve as good descriptors, and that descriptors don't always summarize well the data, what can we do? // First, for the normality issue, // we can try to run a statistical test, the Shapiro-Wilk test, to assert if data is normal. However, this methods has some caveats that we will se later // On the other hand, if we have to do a hypothesis test on data, we can use non-parametric tests // and lastly, if we are unsure of the distribution of the data, it is rarely a mistake to use median and MAD.
							</aside>
							<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
							<span class="fragment">
								<p>Assert normality of data: Shapiro-Wilk test <span style="color: #CC503E">(!!!)</span></p>
							</span>
							<span class="fragment"><p>If not, use <b>non-parametric</b> statistics</p>
								<ul>
									<li>t-test (1 var) $\rightarrow$ Wilcoxon rank test</li>
									<li>t-test (2 var) $\rightarrow$ Mann-Whitney U test</li>

								</ul>
							</span>
							<span class="fragment"><p>Use <b>robust</b> estimators</p>
								<ul>
									<li>Mean $\rightarrow$ median</li>
									<li>Variance (2 var) $\rightarrow$ Mean Absolute Deviation</li>
								</ul>
							</span>
						</section>

						<section >
							<aside class="notes">
								And what about the estimators? Well, if possible, try to graph the data to see if there are outliers, or skews in the distribution.
							</aside>
							<h3 style="text-align: left">Also...</h3>

							<blockquote style="text-align: left">
								&ldquo;...make both calculations and graphs. Both sorts of output should be studied; each will contribute to understanding.&rdquo;
								<p style="text-align: right">F. J. Anscombe, 1973 </p>
							</blockquote> 
							<span class="fragment"> </span>
						</section>

					</section>




					<section>
						<section data-background="#0F8554">
							<h1>V</h1>
							<h2>Low sample numbers</h2>
							<aside class="notes">
								Well, this is obvious, isn't it? But I hear the complaints from the biologists saying: "oh, but we cannot do 200 mice, as you do Computationals (TM)". Yeap, I know. But still, I want to convince you that using 3 mice only might not be the best idea to solve the cancer.
							</aside>
						</section>

						<section data-background="figs/saw.jpg" data-background-opacity=0.8>
							<aside class="notes">
								So, to do that, we'll play a game. Open https://create.kahoot.it/share/guess-the-distribution/3c8a0adc-5ecf-4d77-ae6c-1825b714507c and host the game.
							</aside>
							<h1 style="color: white">Let's play a game</h1>
							<h4 style="color: white">www.kahoot.it  >  Enter game PIN </h4>
						</section>

						<section data-transition="none-out">
							<aside class="notes">

							</aside>

							<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
							<img src="figs/V-nonnormal-gray.svg" class="plain" style="margin-top: -3%">
						</section>

						<section data-transition="none-in none-out">
							<aside class="notes">

							</aside>

							<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
							<img src="figs/V-nonnormal-color1.svg" class="plain" style="margin-top: -3%">
						</section>

						<section data-transition="none-in none-out">
							<aside class="notes">

							</aside>

							<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
							<img src="figs/V-nonnormal-color2.svg" class="plain" style="margin-top: -3%">
						</section>

						<section data-transition="none-in">
							<aside class="notes">

							</aside>

							<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
							<img src="figs/V-nonnormal-color3.svg" class="plain" style="margin-top: -3%">
						</section>

						<section data-transition="none-in">
							<aside class="notes">
								HERE IS THE CODE! I had to say that this distribution of points was not the most probable one, but hey, it appears there.
							</aside>

							<h3>And this is not made up!</h3>
							<pre><code data-trim data-noescape class="python">
								import numpy as np
								import scipy as scp

								for seed in tqdm(range(10000)): #seed is 4327
								n = 15
								np.random.seed(seed)
								N = scp.stats.norm(0,1)
								S2 = scp.stats.skewnorm(2)
								S10 = scp.stats.skewnorm(10)
								Nrvs, S2rvs, S10rvs = N.rvs(n), S2.rvs(n), S10.rvs(n)

								if np.mean(S2rvs) < 0.45 and np.mean(Nrvs) > 0.7:
								break
							</code></pre>
						</section>

						<section data-transition="none-in">
							<aside class="notes">
								Nonetheless, regardless of you guessing capabilities, I'm sure that Shapiro-Wilk test is even worse. For the Ns(10) it needs at least 40 points to start asserting that the distribution is not normal. And for Ns(2) it takes around 250.
							</aside>

							<h3>Shapiro Wilk is even worse at guessing</h3>
							<img src="figs/V-shapiro-N.svg" class="plain" width="45%" vertical-align="middle">
							<img src="figs/V-nonnormal-dists-recolored.svg" class="plain" width="45%" vertical-align="middle" style="margin-top: -2%">

						</section>

						<section data-transition="none-in" style="text-align: left">
							<aside class="notes">
								So, sometimes your variables might not follow a normal distribution and, if the data is not really really different, Shapiro-Wilk may fail to assert its normality if your number of points is small. // So, we want to run an experiment and we have all these problems. What can we do with that? 
							</aside>

							<h3>So...</h3>
							<ul>
								<li>Biological variables might not be normal</li>
								<li>Shapiro-Wilk test fails for low n</li>
							</ul>

							<span class="fragment" style="text-align: center"><p><b>Any solutions?</b></p></span>
						</section>

						<section style="text-align: left">
							<aside class="notes">
								First, if we want to run an experiment we can just assume our data is normal and try to calculate the sample size to run the test. In this case, if we want to run a simple t-test, the number of samples should be that. // But, we usually don't know which is the extent of the difference of means of the two populations and, second, it does not ackowledge the false positive rate.
							</aside>
							<h2>Solution (Assuming normality)<span class="fragment" style="color: #CC503E"> [hard?]</span></h2>
							<span class="fragment"><p>Calculate sample size. If we expect $|\hat{\mu} - \mu| = D$ then, with $\alpha=0.05$ (one tail): </p>
								<p style="text-align: center"> $\small{z_{0.05} = 1.65 \le \frac{\mu - \hat{\mu}}{\hat{\sigma}/\sqrt{n}} = \frac{D \sqrt{n}}{\hat{\sigma}} \iff n \ge  
								\left(\frac{z_{0.05}\hat{\sigma}}{D}\right)^2}$</p>
								<p style="text-align: center">$\color{#cc503e}{2.73\left( \frac{\hat{\sigma}}{D} \right)^2}$</p>
							</span>

							<span class="fragment"><p>We barely ever have that info, and it does not acknowledge $\beta$.</p></span>
						</section>

						<section style="text-align: left">
							<aside class="notes">
								The solution // 1) is to apply a lower alpha, that is, if we are unsure of our results and we are searching for positive results, then it is better to lower tha alpha value. // Second is to include side effects. Some like odd ratio and test statistics help us identify better the range of uncertainty. Other effect sizes usually tell you how appart are the two distributions, and with that you can have a measure of "certainty" of to what extent we'll be able to tell the two populations apart. // 3) Lastly, we should always include the confidence of interval to test the extent of the data. If, for instance, I see a paper with a CI, I can to some extent believe better, or not, if the data are significative. // PASS TO THE NEXT POINT (SKIP WALSH)
							</aside>
							<h2>Solution II<span class="fragment" style="color: #73af48"> [easy]</span></h2>
							<span class="fragment"><p>Apply a lower $\alpha$: 0.01 or 0.005 (and increase the FN rate!) Is it better a FP or a FN?</p>
							</span>
							<span class="fragment"><p>Include effect sizes (Cohen's d, Hedges' g, <b>odds ratio</b>, <b>test statistics</b>).</p></span>
							<span class="fragment"><p>95% Confidence Interval (parametric), 2 and 98 percentiles of Walsh averages (non-parametric).</p>
							</span>
						</section>

						<section style="text-align: left">
							<aside class="notes">

							</aside>
							<h3>Walsh averages</h3>					
							<ul>
								<li>For $a_1, a_2, \cdots, a_n$ measurements, take all $s_{ij} = (a_i + a_j)/2\;\;i \le j$</li>
								<li>Order the list $(s_{11}, s_{12}, \cdots, )$ and take the 2 and 98 percentiles.</li>
							</ul>
							<img src="figs/V-walsh.svg" class="plain" style="text-align: : center">
						</section>
					</section>




					<section>
						<section data-background="#38A6A5">
							<aside class="notes">
								A correlation is spurious when one of the variabes that should not correlate with the other does so, for any reason. It would be the equivalent of a correlation false positive.
							</aside>
							<h1>VI</h1>
							<h2>Spurious correlations</h2>
						</section>

						<section style="text-align: left">
							<aside class="notes">
								So, before speaking of spurious correlation, we will talk about Pearson correlation. Pearson correlation is still the most used correlation method in science, and it usually works well. // However, we all should know that this correlation works granted some assumptions are made // EXPLAIN EACH OPTION  // 
							</aside>
							<h2>Pearson correlation</h2>
							<p>Really useful for common datasets. <span class="fragment">But it has some assumptions. The most troublesome ones are:</span></p>
							<ul>
								<span class="fragment"><li>No outliers</li></span>
								<span class="fragment"><li>Normality</li></span>
								<span class="fragment"><li>Continuity</li></span>
								<span class="fragment"><li>Linearity</li></span>
								<span class="fragment"><li>Random variables come from same distribution</li></span>
							</ul>
						</section>
						<section style="text-align: left">
							<h2>No outliers [Problem]</h2>
							<img src="figs/VI-correlation-pearson.svg" text-align="center" class="plain" width="100%">	
						</section>

						<section style="text-align: left">
							<h3>Solution: Spearmann correlation</h3>
							<span class="fragment"><p>Spearman correlation is calculated on the <b>rank</b> of the variables, not on their values.</p></span>
							<span class="fragment"><p>More robust to <b>outliers</b>, <b>non-linearity</b> and <b>non-normality</b>.</p></span>	
						</section>

						<section style="text-align: left">
							<h3>Solution: Spearmann correlation</h3>
							<img src="figs/VI-correlation-spearman.svg" text-align="center" class="plain" width="100%">	
						</section>

						<section style="text-align: left">
							<h3>Random variables from different distributions</h3>
							<img src="figs/VI-correlation-blobs.svg" text-align="center" class="plain" width="100%">	
						</section>


						<section style="text-align: left">
							<aside class="notes">
								So, what can we do to improve our analysis when doing correlations? First, we should check that data is normal and linear, if not, try Spearmann // We also should look for biases in our data that makes us think it is not monomodal. If so, we should divide the dataset in each unit, and the run the correlation on them. If we want to compare the two distributions, a correlation is not the best option. // 
								Lastly, if there are outliers in our data, we should check if they have a biological value. If not, maybe Spearmann correlation is better.
							</aside>
							<h2>Solution <span class="fragment" style="color: #EDAD08">[medium]</span></h2>
							<span class="fragment"><p>Assert normality and linearity. If not, try Spearmann correlation</p>
							</span>
							<span class="fragment"><p>Assert data is monomodal, or cannot be divided into categories.</p></span>
							<span class="fragment"><p>Outliers: check meaningfulness, else Spearmann</p></span>
						</section>

					</section>







					<section>
						<section data-background="#1D6996">
							<h1>VII</h1>
							<h2>Circular analysis</h2>
							<aside class="notes">
								So, what id circular analyisis? The best explanatio I found was on Wikipedia // 
							</aside>
						</section>

						<section data-background="figs/doggo.gif" data-background-opacity=0.5>
							<h2>What is circular analysis?</h2>
							<aside class="notes">
								The basic Idea of circular analysis is that we usually find a subset of the data more interesting, or that we can filter variables to make two "noisy", uncorrelated characteristics be more similar between them.
							</aside>
							<span><blockquote>
								&ldquo;In statistics, circular analysis is the selection of the details of a data analysis using the data that is being analysed.&rdquo;
								<a href="https://en.wikipedia.org/wiki/Circular_analysis"> Wikipedia </a>
							</blockquote> </span>

						</section>

						<section>
							<aside class="notes">
								As an example, let's imagine that we have a dataset that has two groups, and we start removing some variables so that we can make two discernible groups. In this case I run a simulation to select data so that their corelations are as different as possible. If you select the points correctly, you can achieve that very easily. Then, if you apply any "shady" method to do that selection, you are doing circular analysis.
							</aside>
							<h2>It is easy to find patterns were there are none</h2>
							<span class="fragment"><video src="figs/normal.mp4" width="49%"></video>
								<video src="figs/line.mp4" width="49%"></video>


							</section>


							<section style="text-align: left">
								<aside class="notes">
									So, these are examples of circular analysis commonly done in science.
								</aside>
								<h2>Examples of circular analysis</h2>
								<ul>
									<span class="fragment"><li>Genomic analysis: apply filter until <b>our</b> set of genes appears.</li></span>
									<span class="fragment"><li>Apply filters/thresholds to points that do not "<b>fit the data</b>". </li></span>
									<span class="fragment"><li>In functional magnetic resonance imaging (fMRI) data, pre-processing is often needed, and might be applied incrementally until the analysis 'works'. (Wikipedia)</li></span>
								</ul>
							</section>

							<section style="text-align: left">
								<aside class="notes">
									And, what can we do to solve that? // In machine learning they do something that is really common, and which is "easily" applicable to this problem.
								</aside>
								<h2>Solution <span class="fragment" style="color: #EDAD08">[medium]</span></h2>
								<span class="fragment">Let's give a glance at machine learning (they have learned to do it well).</span>
							</section>


							<section>
								<aside class="notes">
									One of ML typical problems is regression. That, is, find a function that fits those points in screen. A good regression should allow us to infer the Y-value given any X-values as good as possible. So, we have these points here and, what do we do then to do a regression? // ->
								</aside>
								<img src="figs/ML-example-0.svg" class="plain" width="40%">
							</section>


							<section>
								<aside class="notes">
									Well, for this data points we can adjust as many functions as we want. Here, for instance, we have fitted a line, a curve and a polynomial. In order to measure the goodness of fit, we measure a distance. Here the distance is the Y-distance between the function and the point. The final distance is the sum of distances for each point. So, for that case, the function that has the smallest error for our dataset is the polynomial. BUT, is it really the best?
								</aside>
								<img src="figs/ML-example-1.svg" class="plain" width="100%">
							</section>


							<section>
								<aside class="notes">
									Let's imagine that we add these three points here.
								</aside>
								<img src="figs/ML-example-1.5.svg" class="plain" width="40%">
							</section>

							<section>
								<aside class="notes">
									And now we also measu the distance between the points and the already predicted function.
								</aside>
								<img src="figs/ML-example-2.svg" class="plain" width="100%">
							</section>


							<section>
								<aside class="notes">
									If we now compare the distance of the training set (gray) and test set (colors), we have three main options:
									EXPLAIN THEM 
								</aside>
								<img src="figs/ML-example-3.svg" class="plain" width="100%">
							</section>

							<section style="text-align: left">
								<h2>So... how can we apply this?</h2>
								<span class="fragment">If there are enough technical replicates / data points...</span>
								<span class="fragment">subset the data into "training" / "test" sets (60/40 or 70/30).</span>

								<span class="fragment"><p></p>Consider the results as "valid" if they replicate in test set.</span>
							</section>
						</section>





						<section>
							<section data-background="#5F4690">
								<h1>VIII</h1>
								<h2>Failing to correct for <br> multiple comparisons</h2>
							</section>

							<section>
								<h2 style="text-align: left" >One statistic test at a time!</h2>
								<span class="fragment"> 
									<p style="text-align: left">We <b>must not</b> forget that one test is for a specific variable: a gene, a voxel, an experiment... </p>
									<img src="figs/p-value.svg" class="plain", style="width: 65%" align="middle"></span>
								</span>


							</section>

							<section data-transition="none-out">
								<aside class="notes">
									To put you an example of the relevance of multiple testing, let's imagine that we measured the gene expression of two CONTROL groups, and for each gene we did a t-test. Those are the t-values. Then, t-value distribution will always be tailed, and under the H0 testing we should say that very low or vey high t-values should be "selected" as "significant" for the testing.
								</aside>
								<h3 style="text-align: left">A theoretical example</h3>
								<img src="figs/VIII-array-gray.svg" class="plain", style="width: 100%" align="middle"></span>
							</section>

							<section data-transition="none-in none-out">
								<aside class="notes">
									If we set an alpha of 0.05 all colored dots would be selected as significantly expressed, when in reality they belong to the control population. If we order the values...
								</aside>
								<h3 style="text-align: left">A theoretical example</h3>
								<img src="figs/VIII-array-color.svg" class="plain", style="width: 100%" align="middle"></span>
							</section>

							<section data-transition="none-in">
								We would see that there would be 2005 genes with significant differential expression at 0.05, and 190 if we set alpha at 0.005. And this is indeed normal, because the alpha indeed indicates the probability of saying that a gene is a statistically significant DEG when in reality it is not.
								<h3 style="text-align: left">A theoretical example</h3>
								<img src="figs/VIII-array-color-sorted.svg" class="plain", style="width: 100%" align="middle"></span>
							</section>

							<section data-background="https://media0.giphy.com/media/5NAWDQEPeFFle/giphy.gif?cid=790b761116c13fb79de8972d0ac86f4715dae09ae5942a18&rid=giphy.gif" 
							data-background-opacity="0.4">
							<h3 style="text-align: left">A practical example: a salmon in a social environment</h3>

							<a href="https://teenspecies.github.io/pdfs/NeuralCorrelates.pdf" ><p style="font-size: 80%"> Bennett CM, Baird AA, Miller MB, Wolford GL. Neural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: an argument for proper multiple comparisons correction
								. <b> J Serendipitous Unexpected Results </b> 2009
							; 1: 1–5.</a>
						</section>

						<section data-transition="none-out">
							<img src="figs/salmon1.png" class="plain", style="width: 80%" align="middle"></span>
							<img src="figs/salmon2.png" class="plain", style="width: 80%" align="middle"></span>
						</section>

						<section data-transition="none-out">
							<img src="figs/salmon3.png" class="plain", style="width: 40%" align="middle"></span>
							<img src="figs/salmon4.png" class="plain", style="width: 70%; margin-top: -2%" align="middle"></span>
						</section>

						<section style="text-align: left">
							<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
							<span class="fragment">
								<ul>
									<span class="fragment"><li>Be aware that we are multitesting.</li></span>
									<span class="fragment"><li>Apply more stringent p-values (GWAS: $\sim 10^{-8}$).</li></span>
									<span class="fragment"><li>Apply multiple test correction.
										<ul>
											<span class="fragment"><li>Bonferroni: $\alpha^* = \frac{\alpha}{N}$. <span style="color: #909090">(Maybe too stringent)</span></li>
											<li>Benjamini-Hochberg (aka FDR).</li></span>
										</ul>
									</li></span>
								</ul>
							</span>							
						</section>
					</section>



					<section>
						<section data-background="#94346E">
							<h1>IX</h1>
							<h2>Over-interpreting <br> non-significant results</h2>
						</section>

						<section style="text-align: left">
							<aside class="notes">
								Can we assume that if the test turned out non-significant, then our experiment is negative? // Well, the test may turn out negative due to several reasons. // EXPLAIN IMAGE > UP = TOO MUCH NOISE / DOWN = SMALL DATASET
							</aside>
							<h3>What happens when p > 0.05?</h3>
							<span class="fragment">
								The test is negative, there is no effect. 
							</span>
							<span class="fragment">
								Well, not always. <p></p> It can imply that...
							</span>
							<ul>
								<li class="fragment">
									there is too much noise (resolutive power of technique not enough)
								</li>
								<li class="fragment">
									the dataset is too small
								</li>
							</ul>

							<span class="fragment">
								<img src="figs/IX-p.svg" class="plain">
							</span>
						</section>
						<section>
							<aside class="notes">
								Indeed if we recall the slides from the beginning, apart from looking at the positives, we may need to look also at the negtives. In that case, we should look at the power. In both cases, the power is quite small.
							</aside>
							<img src="figs/H0H1.svg" class="plain" width="40%" style="margin-bottom: -2%">
							<img src="figs/IX-p.svg" class="plain" width="80%" style="margin-bottom: -2%">
							<img src="figs/IX-p-dists.svg" class="plain" width="70%" style="margin-bottom: -2%">
						</section>
						<section style="text-align: left">
							<aside class="notes">

							</aside>
							<h2>Solution <span class="fragment" style="color: #EDAD08">[medium]</span></h2>
							<span class="fragment"><p>Assess the minimum $n$ and the statistical power before the analysis.</p>
							</span>
							<span class="fragment"><p>If not sure (or not possible), do not over-interpret negative results.</p></span>
						</section>
					</section>






					<section>
						<section data-background="#6F4070">
							<h1>X</h1>
							<h2>Correlation <span style="color: #000000"> <p> does not imply </p></span> causation!</h2>
						</section>


						<section style="text-align: left">
							<aside class="notes">

							</aside>
							<h3>Finding correlations is easy</h3>
							<span class="fragment">
								<h3 style="text-align: right">explaining their cause is not.</h3>
								<p style="color: #909090; text-align: center; margin-top: 7%" > Let's see some examples.</p>
							</span>
						</section>


						<section>
							<aside class="notes">

							</aside>
							<img src="figs/X-chart1.svg" class="plain" style="width: 80%">
							<img src="figs/X-chart2.svg" class="plain" style="width: 80%; margin-top: -2%">
						</section>


						<section>
							<aside class="notes">

							</aside>
							<h3 style="text-align: left;">Nobel awards VS ... </h3>  
							<img src="figs/X-Nobel-grafica.png" class="plain" style="width: 100%">
							<a href="https://academic.oup.com/jn/article/143/6/931/4571741" > <p style="text-align: right; font-size: 0.5em" >10.3945/jn.113.174813</p>
							</a>
						</section>


						<section>
							<aside class="notes">

							</aside>
							<img src="figs/X-real-example.png" class="plain" style="width: 80%">
						</section>


						<section>
							<aside class="notes">

							</aside>
							<h3 style="text-align: left;">It all boils down to... </h3> <h3 style="text-align: right" class="fragment">confounding variables. </h3> 
							<span class="fragment"><img src="figs/X-corr-cas.svg" class="plain" style="width: 40%"></span>
							<span class="fragment"><p>Maybe more talkative / social people are perceived as more attractive, and also find better jobs? </p></span>
						</section>


						<section>
							<aside class="notes">

							</aside>
							<h3 style="text-align: left;">A more important case<span style="text-align: right" class="fragment">: smoking causes cancer</span></h3> 
							<span class="fragment" style="text-align: left;"><p style="font-size: 80%">Maybe a better diagnosis implied a higher detection rate.</p></span>
							<span class="fragment" style="text-align: left;"><p style="font-size: 80%">Maybe people genetically predispose to smoking have higher chances of lung cancer.</p></span>
							<span class="fragment" style="text-align: left;"><p style="font-size: 80%">Maybe industrial development incited easyness to tobacco, and easyness to cars $\rightarrow$ cancers are cause by pollution in general.</p></span>
							<span></span>
							<span class="fragment"><img src="figs/X-smoke-cancer.png" class="plain" style="width: 60%; margin-bottom: -3%"></span>

						</section>

						<section style="text-align: left">
							<h2>Solution <span class="fragment" style="color: #CC503E">[hard]</span></h2>
							<span class="fragment"><p>Be aware of possible underlaying confounding variables, and try to detect them</p>
							</span>
							<span class="fragment"><p>If confounding variables are found, try to adjust data.</p>
							</span>
							<span class="fragment"><p>Some methods exist to determine causality (for time series): Convergent Cross Mapping, Granger causality test.</p>
							</span>
							<span class="fragment"><p>If unsure, present conclusions as tentative</p></span>
						</section>
					</section>






					<section>
						<section data-background="#994E95">
							<h1>.</h1>
							<h2>Conclusions</h2>
						</section>

						<section>
							<aside class="notes">
								Given the amount of *only* statistical reasons a experiment might go awry, it is not uncommon to suffer from a 
								reproducibility crisis. Indeed, nature did a poll to 1576 scientists and found that 52% of them believed there was a significant reproducibility crisis, and 38% believed there was a slight crisis. We indeed can see here that, regardless of discipline, more than half of the scientists failed to replicate either their own or else's work. 
								Poor statistical analysis is one of the main causes for that outcome, but we should consider more important others as well, such as the pressure to publish or the insufficient mentoring.

							</aside>
							<h3 style="text-align: left;">Reproducibility matters</h3> 
							<img src="figs/nature-reproducibility.png" class="plain" style="width: 100%">
							<p style="text-align: right"><a href="https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970" style="font-size: 0.5em; ¡">1,500 scientists lift the lid on reproducibility</a></p>
						</section> 

						<section>
							<aside class="notes">
								Take home message:

							</aside>
							<h2>Take home message</h2> 
							<h4 class="fragment">Sometimes science is a b*tch </h4>
							<h4 class="fragment">statiticians can't do magic</h4>
							<span class="fragment"> <h4>and when they try, they end up like this</h4>
								<img src="figs/monkey.gif" class="plain"> </span>
							</section> 
						</section>

						<section>
							<section data-background="#94346E">
								<h1>Thanks!</h1>
							</section>
						</section>


					</section>
				</div>
			</div>

			<script src="js/reveal.js"></script>

			<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				hash: true,
				controls: true,
				dependencies: [
				{ src: 'plugin/markdown/marked.js' },
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/highlight/highlight.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/math/math.js', async: true },
				{ src: 'plugin/reveal.js-menu/menu.js' }
				],
			});
			Reveal.configure({ slideNumber: true });
		</script>
	</body>
	</html>
