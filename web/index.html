<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>reveal.js</title>

	<link rel="stylesheet" href="css/reset.css">
	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/mytheme.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/monokai.css">

	<!-- For custom controls -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement( 'link' );
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>
</head>
<body>
	<div class="reveal">
		<div class="slides">
			<section id="titulo-y-paper">
				<section style="width: 100%">
					<h2 style="text-align: left; color: #8c0d04">
						Common stats mistakes from (and for) scientists
					</h2>
					<hr>
					<table style="width:100%; padding-left: 0%">
						<tr style="color: #909090; ">
							<th style="font-weight: lighter; padding-left: 0%; padding-top: 10%">
								Alex M. Ascensión
							</th>
							<th style="font-weight: lighter; padding-right: 0%; padding-top: 10%; text-align: right">
								<span id="datetime"></span>

								<script>
									var dt = new Date();
									document.getElementById("datetime").innerHTML = dt.toLocaleDateString();
								</script>
							</th>
						</tr>
					</table>

					<aside class="notes">
						This is a presentation I've been thinking of for a long time. I won't be explaining my results, as in the typical wetlab presentation, but I will focus on common mistakes scientists, and non-scientists, do when doing the analysis of their experiments.
					</aside>
				</section>

				<section id="portada imagen">
					<img class="plain" src="figs/portada.png">

					<aside class="notes">
						The main source of inspiration is this paper. In this paper they explain ten common statistical mistakes often done in academia, and which should be accounted for to improve the analysis. In this presentation I will inlude most of them, so it might be a bit lengthy.
					</aside>
				</section>

				<section id="portada imagen">
					<iframe width="80%" height="600" src="https://www.youtube.com/embed/Hz1fyhVOjr4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

					<aside class="notes">
						To begin with, I would like to show you a video that Olga showed me a while ago. It might be a satirical representation of the interaction between biologists and statisticians, but this kind of situation does indeed commonly happen. // In this presentation I will guide towards the mathematical or logical mistakes that we usually make when doing data analysis or performing an experiment. BUT, as we will see later, other SOCIAL factors are relevant for good science.
					</aside>
				</section>
			</section>
			

			<section>
				<section data-background="#94346E">
					<h1>.</h1>
					<h2>101 STATS</h2>
					<aside class="notes">
						Just before we start, I want to make sure that some basics are known. I guess that you all remember the statistical tests and all this stuff, but I will do some refreshing, because great part of the presentation will be based on that, as you can imagine.
					</aside>
				</section>

				<section>
					<h2>Descriptors: median (M) and Median Absolute Deviation (MAD)</h2>
					$\small{\text{Mean: } \mu = \frac{ 1}{N}\sum_i^N x_i \qquad \text{Variance: } \sigma^2 =  \frac{1}{N}\sum_i^N(x_i - \mu)^2}$
					<img src="figs/median-MAD.png" class='plain' style="width: 35%">
					<aside class="notes">
						Before I talk about tests, I want to make sure we know about some statistical descriptors. We know what mean and standard deviation are. The idea of the mean is to find the "balance" point of the distribution, and the std tries to see how far from the mean the data lays. There are two other commonly used descriptors, equivalent to the mean and std: the median, and the MAD (we will say why they are important later). ** In the MAD, we SUBSTRACT THE MEDIAN FROM THE SAMPLES, as similar to the std** We will explain why they are important later, but simpy put, those descriptors are more robust in the sense that if I replace one of the numbers for other (6 for 10, 1 for 0, etc.) the descriptors should not change that much.
					</aside>
				</section>

				<section>
					<h2>Distributions</h2>
					<table align="center" valing="center" style="padding: 0">
						<tr>
							<td  align="center" style="padding: 0; vertical-align: middle" >
								<img src="figs/H0H1.svg" class='plain' style="margin: 1">
							</td>
							<td align="center" style="padding: 0; vertical-align: middle">
								<img src="figs/H0H1-table.svg" class='plain' style="height: 70%">
							</td>
						</tr>

					</table>

					<aside class="notes">
						In this slides I will talk abouit two SIMILAR BUT SEPARATE THINGs. First, I want you to recall the famous alpha value, what is it, and also want you to understand the beta and 1-beta values. **Explain the two distributions about diabetes -> from the explain FP / FN etc -> from there explain alpha and beta**. RECALL THAT 1-beta WILL BE IMPORTANT IN THE REST OF PRESENTATION. As for hypotesis testing, *explain how contrasts are formulated* 
					</aside>
				</section>

				<section>
					<h2>Inference tests</h2>
					<p style="text-align: center; %">$\begin{cases}
						\scriptsize{ H_0: \hat{\mu}_{H_1} \le / = \mu_{H_0} }\\
						\scriptsize{ H_1: \hat{\mu}_{H_1} >/\neq \mu_{H_0} }
					\end{cases} \rightarrow t, F, \chi^2,\cdots \rightarrow p\text{ - value}$</p>

					<img src="figs/H0H1.svg" class='plain' width="40%" style="margin-bottom: 4%">
					<img src="figs/p-value.svg" width="50%" class="plain" style="margin-left: 2%">
					<aside class="notes">
						The second part is the iference test. With the example above, for instance we want to test whether the set of diabetics and the non-diabetics have different glucose levels. In order to do that, we have to transform this "notion" of difference into a number. This number is called stadistic. The fun thing about statistics is that THEY HAVE BEEN CRAFTED TO FOLLOW A CERTAIN DISTRIBUTION. So, why does the statistic follow that distribution? Suppose we take 10 random numbers from H0, and other 10 random numbers from H0, then compute the function for t. Now take other 10 points and other 10 points, it will give another point. The "sum" of all possible combinations of 10 numbers and 10 numbers produces a distribution, which is the distribution of the statistic. THUS, if we take 10 points from H0 and, NOW, 10 points from H1, where does the t value lay on? The more different the points, the more to the right or the left of the distribution they will go. So, if a point is far right, it can be because we have chosen two sets of points from H0 that are very different, or a set of H0 and a set of H1. Which is more likely? That's what we set the alpha for.  // SO now that we have grasped the basics of stats, we'll move on to see how can we fail using statistics!
					</aside>
				</section>



			</section>

			<section>
				<section data-background="#CC503E">
					<h1>I</h1>
					<h2>Absence of [quality] <br> control group</h2>
				</section>

				<section style="text-align: left">
					<aside class="notes">
						What do we want to say with QC? Let's put an example. Say we want to study the effect of a drug on schizofrenic patients. // 

						Then we will have a experimental group with certain variables like weight // height // age // and other variables. Among them, the fact that they are schizofrenic and that they are taking the drug. // 

						In an ideal control, we would like a set of individuals that have the same values for all the variables but one: the drug. In this case, they should be taking a placebo. // 

						BUT, the common thing is that, when designing an experiment, the variables don't have the same values. Indeed, there are a set of common problems regarding controls that are not usually taken into account: // ->
					</aside>
					<!-- <h3 style="padding-bottom: 5%">The problem</h3> -->
					<div>
						<p> <span class="fragment">Test</span></p>
						<p>
							<span class="fragment">$\stackrel{\text{weight}}{\normalsize  A}\;$</span>
							<span class="fragment">$\stackrel{\text{height}}{\normalsize B}\;$</span>
							<span class="fragment">$\stackrel{\text{age}}{\normalsize C}\;$</span>
							<span class="fragment"> $\cdots\;$
								$\stackrel{\text{income}}{\normalsize X}\;$
								$\stackrel{\text{schizophrenia}}{\normalsize Y}\;$
							$\stackrel{\text{drug X}}{\normalsize Z}$ </span>
						</p>

						<span class="fragment">
							<p style="padding-top: 1%"> Control [ideal]</p>
							<p>
								$\stackrel{\text{weight}}{\normalsize A}\;$
								$\stackrel{\text{height}}{\normalsize B}\;$
								$\stackrel{\text{age}}{\normalsize C}\;$
								$\cdots\;$
								$\stackrel{\text{income}}{\normalsize X}\;$
								$\stackrel{\text{schizophrenia}}{\normalsize Y}\;$
								$\stackrel{\text{placebo}}{\normalsize \color{#8c0d04}{Z^*}}$ 
							</p>
						</span>

						<span class="fragment">
							<p style="padding-top: 1%"> Control [real]</p>
							<p>
								$\stackrel{\text{weight}}{\normalsize A}\;$
								$\stackrel{\text{height}}{\normalsize \color{#8c0d04}{B^*}}\;$
								$\stackrel{\text{age}}{\normalsize C}\;$
								$\cdots\;$
								$\stackrel{\text{income}}{\normalsize \color{#8c0d04}{X^*}}\;$
								$\stackrel{\text{schizophrenia}}{\normalsize Y}\;$
								$\stackrel{\text{placebo}}{\normalsize \color{#8c0d04}{Z^*}}$ 
							</p>
						</span>
					</div>
				</section>

				<section style="text-align: left">
					<aside class="notes">
						In social experiments, sometimes experimenters are not blinded (they explain the aim of the study), and this impacts the results // 

						Sometimes control and experiment groups are sampled at different times, and time should be considered another variable on the experiment // 

						Sometimes the number of samples in the control set and the experimental set are not the same. I will not talk about this on the presentation, but this is really
						common on Big Data analysis. For instance, we want to predict whether a breast image contains a cancer or not. In this case, it is common that there are many more images without cancer, and this usually is a problem for the algorithms. // 

						An the most important one: many times all variables are not considered, and this can bias the experiment for a certain case. I will show you two examples on that: // ->
					</aside>
					<h2>Common problems</h2>
					<ul>
						<span class="fragment"><li>Experimenters are not blinded</li></span>
						<span class="fragment"><li>Control & exp. groups sampled at different times</li></span>
						<span class="fragment"><li>$N_{\text{control}} \neq N_{\text{exp}}$</li></span>
						<span class="fragment"><li>All biological <b>heterogeneity</b> is <b>not considered</b> in the experimental / control group.</li></span>

					</ul>
				</section>

				<section style="text-align: left" data-background="figs/dummy.jpg" data-background-opacity=0.2>
					<aside class="notes">
						All produced cars require a security test. Usually this tests consist of a dummy inside the car, and the car crashes. Based on the results we can infer the security of the car.
						During decades, the dummies have been designed to consider an average male, and therefore some features of the cars, like the position of security belt, or airbags, has been adjusted for that dummy. As a result...  CITE
					</aside>

					<h3>Some examples (I)</h3>
					<p>Dummies are designed as average <b>male</b> (based on height/weight) </p>
					<blockquote>
						&ldquo;The fatality risk of a female driver is an estimated 17.0 ± 1.5 percent higher than for a male of the same age, given similar physical insults.&rdquo;
						<a href="https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/811766"> NHTSA </a>
					</blockquote> 
				</section>

				<section style="text-align: left" data-background="figs/ambien.jpg" data-background-opacity=0.15>
					<aside class="notes">
						The second case is of a drug, Ambien. Ambien is a sedative // When the drug was approved, the clinical trials featured "typical" caucasic men. // However, women metabolize ambien more slowly, so the drug does its effect more time. Many women reported that they still felt groggy while driving early in the morning, and in some cases traffic accidents happened // As a result, FDA recommended halving the intake of Ambien for women.
					</aside>

					<h3>Some examples (II)</h3>
					<span class="fragment"><p>Ambien (zolpidem) is a sedative.</p></span> 
					<span class="fragment"><p>Clinical trials were done in caucasic men, age 30, weight $\sim$ 70 kg.</p></span> 
					<span class="fragment"><p>Women metabolize ambien more slowly $\rightarrow$ women driving to work still groggy.</p></span> 
					<span class="fragment"><p>FDA recommended halving the intake (<a href="https://www.nytimes.com/2013/01/11/health/fda-requires-cuts-to-dosages-of-ambien-and-other-sleep-drugs.html">source</a>).</p></span> 
				</section>

				<section style="text-align: left">
					<h2>Solution <span class="fragment" style="color: #CC503E">[hard]</span></h2>
					<span class="fragment"><p>Be aware of as many variables as possible</p>
					</span>
					<span class="fragment"><p>Present conclusions as tentative</p></span>
				</section>
			</section>





			<section>
				<section data-background="#EDAD08">
					<aside class="notes">
						Como concepto básico de estadística tenemos los grados de libertad. Es hablo de los grados de libertad por las equivocaciones típicas que hay con ellos, además de que 
						otros fallos típicos también están asociados a los grados de libertad.
					</aside>
					<h1>II</h1>
					<h2>Inflating the degrees <br> of freedom</h2>
				</section>

				<section style="text-align: left">
					<aside class="notes">
						DEFINIR GRADO DE LIBERTAD // Por ejemplo, suponemos que queremos estimar la media de la altura de una población y tenemos 10 personas.
						Los grados de libertad son 9 porque la altura de la décima persona puede sacarse en base a la altura de las 9 personas y la media.
					</aside>
					<h2>What is a degree of freedom?</h2>
					<span class="fragment"><p>N of <b>independent</b> pieces of information to calculate the estimate</p>
					</span>
					<ul>
						<span class="fragment"><li>DoF 1 sample: $N-1$</li></span>
						<span class="fragment"><li>DoF 2 samples: $N_1 + N_2 - 2$</li></span>
					</ul>
				</section>	

				<section style="text-align: left">
					<aside class="notes">
						Vamos a simular datos normales. En este caso, van a tener media 0.5 y desviación 1. Queremos ver si la media de la población 
						es mayor que 0.
					</aside>

					<h2>Why are DoF important?  </h2><h4>(1 sample)</h4>
					<span class="fragment">
						We are going to simulate normal data: $X \sim N(\mu_X, \sigma_X)$.
						We want to test whether $\mu_X > 0$. </span>
						<span class="fragment"> <p style="text-align: center; padding-top: 5%">$\begin{cases}
							H_0: \hat{\mu}_X \le 0 \\
							H_1: \hat{\mu}_X > 0

						\end{cases}$</p></span>

					</section>	

					<section >
						<aside class="notes">
							Suponemos dos casos. En el caso de la izquierda tenemos 20 puntos, luego los grados de libertad son 19. En el 
							caso de la derecha tenemos que hemos duplicado los puntos para tener un total de 40, luego los grados de libertad
							son 39. En este caso, por ejemplo, la distribución de la izquierda no aseguramos que tenga media mayor que 0, pero en 
							la de la derecha sí.
						</aside>
						<h2 style="text-align: left">Why are DoF important?  </h2><h4 style="text-align: left">(1 sample)</h4>
						<img src="figs/II-pvalues-20VS40.svg" text-align="center" class="plain" width="65%">						
					</section>	

					<section >
						<aside class="notes">
							Ahora extendemos el caso a varias N. Vamos a suponer que simulamos 1000 veces una población con media 0, 0.5, y 1. 
							El contraste de hipótesis anterior se cumple en 0.05 para mu = 0.5 en N = 30. Entonces si tomamos duplicamos el 
							DoF, con hasta N = 30 sufrimos ese fenómeno, y el resultado del contraste sería falso.
						</aside>
						<h2 style="text-align: left">Why are DoF important?  </h2><h4 style="text-align: left">(1 sample)</h4>
						<img src="figs/II-pvalues-N.svg" text-align="center" class="plain" width="65%">								
					</section>	

					<section >
						<aside class="notes">
							Vamos a practicar esto con dos ejemplos tipicos que suelen fallarse.
						</aside>
						<h2 style="text-align: left">Extending DoF to 2 samples.</h2>
						<span class="fragment">
							<img src="figs/II-DROGA+PLACEBO.svg" text-align="center" class="plain" width="100%">	
						</span>


						<div class="row">
							<div class="column"><span class="fragment"><p>What is the DoF?</p></span></div>
							<div class="column"><span class="fragment"><p>20 - 2 = 18</p>
								<p>$\tiny{\begin{cases}
									H_0: \mu_A = \mu_B\\
									H_1: \mu_A \neq \mu_B
								\end{cases}}$</p></span>
							</div>
						</div> 
					</section>	


					<section >
						<aside class="notes">
							Vamos a practicar esto con dos ejemplos tipicos que suelen fallarse. En este primer caso tenemos 10 personas a las que se les da un placebo
							y 10 a las que se les da una droga. Cual es la $N$ 20 - 2 = 18
						</aside>
						<h2 style="text-align: left">Extending DoF to 2 samples.</h2>
						<span class="fragment">
							<img src="figs/II-DROGA+TIEMPO.svg" text-align="center" class="plain" width="100%">	
						</span>


						<div class="row">
							<div class="column"><span class="fragment"><p>What is the DoF?</p></span></div>
							<div class="column"><span class="fragment"><p>10 - 1 = 9</p>
								<p>$\tiny{D = x_t - x_0\;\;\begin{cases}
									H_0: \mu_D = 0\\
									H_1: \mu_D \neq 0
								\end{cases}}$</p></span>
							</div>
						</div> 
					</section>	


					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
						<span class="fragment"><p>Adapt to DoF and rerun the test.</p></span>
						<span class="fragment"><p>If results become not significant, present them as tentative.</p></span>
					</section>

				</section>

				<section>
					<section data-background="#E17C05">
						<aside class="notes">
							En el segundo caso tenemos que a 10 personas se les mide la variable, se les hace tomar un medicamento, y luego se les mide la variable.
							Entonces N = 9.
						</aside>
						<h1>III</h1>
						<h2>Interpret <div style="color: #FFFFFF">comparisons</div> between groups <div style="color: #FFFFFF">without comparing</div> them</h2>
					</section>

					<section>
						<aside class="notes">
							En este primer caso, imaginemos que hacemos dos experimentos y nos salen las dos correlaciones. EXPLICAR P y R!!!! Si las analizamos por separado, tenemos que
							la correlación azul, por ejemplo, tiene una correlacion positiva, mientras que la correlación roja es negativa. Entonces, lo lógico sería pensar que los datos
							provienen de dos distribuciones diferentes. Sin embargo, si hacemos un test de Kolmogorov-Smirnov de 2D, nos da que ambas muestras pueden provenir de la misma 
							distribución.
						</aside>
						<img src="figs/III-corrs.svg"  class="plain" style="width: 88%">
					</section>
					<section>
						<aside class="notes">
							En este segundo caso planteamos una situación distinta. Supongamos que tenemos dos experimentos en los que introducimos una perturbación. 
							Por cada experimento queremos testear que, por ejemplo, un medicamento ha aumentado los niveles de cortisol en sangre. El farmaco A, por la hipótesis, 
							cumple que ha aumentado los niveles, pero el B no. Entonces A y B actuan diferentemente. Sin embargo, si hacemos un t test entre ambas muestras, el test sale
							que ambos fármacos no son diferentes.
						</aside>
						<img src="figs/III-ttest.svg"  class="plain" style="width: 88%">
						<p style="text-align: center; margin-top: -2%">$\tiny{\begin{cases}
							H_0: \mu_X \le 0 \\
							H_1: \mu_X > 0
						\end{cases}}$</p>
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
						<span class="fragment"><p>Avoid comparing statistics from different tests at once</p>
						</span>
						<span class="fragment"><p>Run tests to compare the two effects (e.g. ANOVA)</p></span>
					</section>
				</section>



				<section>
					<section data-background="#73AF48">
						<h1>IV</h1>
						<h2>Data is not normal</h2>
					</section>



					<section style="text-align: left">
						<h3>Many biological variables are not normal!</h3>
						<img src="figs/portada-nonnormal.png" class='plain' style="margin-top: -2%; margin-bottom: -2%">
						<p>231 curated papers with non-normal data: Gamma, Negative Binomial, Binomial, etc.</p>
					</section>

					<section style="text-align: left">
						<p>In transcriptomics, most of the data is skewed too! </p>
					</section>

					<section style="text-align: left">
						<h2>Anscombe's quartet</h2>
						<p>Datasets from different distributions can share similar descriptors!</p>
						<span class="fragment"><img src="figs/anscombe.png" class='plain' ></span>
					</section>

					<section style="text-align: left">
						<h2>Anscombe's quartet</h2>
						<p>Datasets from different distributions can share similar descriptors!</p>
						<table style="width:35%; font-size: 25px">
							<tr><td><span style="color: #73AF48">$\mu_X$: 9</span></td></tr>
							<tr><td><span style="color: #73AF48">$\sigma_X$: 11</span></td></tr>
							<tr><td><span style="color: #73AF48">$\mu_Y$: 7.5</span></td></tr>
							<tr><td><span style="color: #73AF48">$\sigma_Y$: 4.125</span></td></tr>
							<tr><td><span style="color: #73AF48">Correlation $X,Y$: 0.816</span></td></tr>
							<tr><td><span style="color: #73AF48">Linear regression: $y = 3 + 0.5x$</span></td></tr>
							<tr><td><span style="color: #CC503E">Median</span></td></tr>
							<tr><td><span style="color: #CC503E">Median Absolute Deviation</span></td></tr>
						</table> 
					</section>

					<section style="text-align: left">
						<h2>Datasaurus!</h2>
						<span class="fragment"><img src="figs/datasaurus.png" class='plain' style="width: 75%; text-align: center"></span>
					</section>

					<section style="text-align: left">
						<h3>Therefore...</h3>
						<span class="fragment"><p>beware of the mean, variance, correlation of datasets!</p></span>
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
						<span class="fragment">
							<p>Assert normality of data: Shapiro-Wilk test <span style="color: #CC503E">(!!!)</span></p>
						</span>
						<span class="fragment"><p>If not, use <b>non-parametric</b> statistics</p>
							<ul>
								<li>t-test (1 var) $\rightarrow$ Wilcoxon rank test</li>
								<li>t-test (2 var) $\rightarrow$ Mann-Whitney U test</li>

							</ul>
						</span>
						<span class="fragment"><p>Use <b>robust</b> estimators</p>
							<ul>
								<li>Mean $\rightarrow$ median</li>
								<li>Variance (2 var) $\rightarrow$ Mean Absolute Deviation</li>
							</ul>
						</span>
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h2>Are robust estimators better?</h2>
						<span class="fragment"><p>If the data is not normal or has outliers, <b>yes</b>!</p>
							<p>Let's see some examples.</p>
						</span>
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h3>Robust estimators for non-normal distributions</h3>
						<img src="figs/IV-mean-M-dists.svg" class="plain">
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h3>Robust estimators are better with outliers</h3>
						<img src="figs/IV-meanMAD-scatter.svg" class="plain">
					</section>
				</section>




				<section>
					<section data-background="#0F8554">
						<h1>V</h1>
						<h2>Low sample numbers</h2>
					</section>

					<section data-background="figs/saw.jpg" data-background-opacity=0.8>
						<aside class="notes">

						</aside>
						<h1 style="color: white">Let's play a game</h1>
						<h4 style="color: white">www.kahoot.it  >  Enter game PIN </h4>
					</section>

					<section data-transition="none-out">
						<aside class="notes">

						</aside>
						
						<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
						<img src="figs/V-nonnormal-gray.svg" class="plain" style="margin-top: -3%">
					</section>

					<section data-transition="none-in none-out">
						<aside class="notes">

						</aside>
						
						<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
						<img src="figs/V-nonnormal-color1.svg" class="plain" style="margin-top: -3%">
					</section>

					<section data-transition="none-in none-out">
						<aside class="notes">

						</aside>
						
						<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
						<img src="figs/V-nonnormal-color2.svg" class="plain" style="margin-top: -3%">
					</section>

					<section data-transition="none-in">
						<aside class="notes">

						</aside>
						
						<img src="figs/V-nonnormal-dists.svg" class="plain" style="margin-top: -2%">
						<img src="figs/V-nonnormal-color3.svg" class="plain" style="margin-top: -3%">
					</section>

					<section data-transition="none-in">
						<aside class="notes">

						</aside>
						
						<h3>And this is not made up!</h3>
						<pre><code data-trim data-noescape class="python">
							import numpy as np
							import scipy as scp

							for seed in tqdm(range(10000)): #seed is 4327
							n = 15
							np.random.seed(seed)
							N = scp.stats.norm(0,1)
							S2 = scp.stats.skewnorm(2)
							S10 = scp.stats.skewnorm(10)
							Nrvs, S2rvs, S10rvs = N.rvs(n), S2.rvs(n), S10.rvs(n)

							if np.mean(S2rvs) < 0.45 and np.mean(Nrvs) > 0.7:
							break
						</code></pre>
					</section>

					<section data-transition="none-in">
						<aside class="notes">

						</aside>
						
						<h3>Shapiro Wilk is even worse at guessing</h3>
						<img src="figs/V-shapiro-N.svg" class="plain" width="45%" vertical-align="middle">
						<img src="figs/V-nonnormal-dists-recolored.svg" class="plain" width="45%" vertical-align="middle" style="margin-top: -2%">

					</section>

					<section data-transition="none-in" style="text-align: left">
						<aside class="notes">

						</aside>
						
						<h3>So...</h3>
						<ul>
							<li>Biological variables might not be normal</li>
							<li>Shapiro-Wilk test fails for low n</li>
						</ul>

						<span class="fragment" style="text-align: center"><p><b>Any solutions?</b></p></span>
					</section>

					<section style="text-align: left">
						<aside class="notes">
							The problem is that even this solution is not the perfect because it does not
						</aside>
						<h2>Solution (Assuming normality)<span class="fragment" style="color: #CC503E"> [hard?]</span></h2>
						<span class="fragment"><p>Calculate sample size. If we expect $|\hat{\mu} - \mu| = D$ then, with $\alpha=0.05$ (one tail): </p>
							<p style="text-align: center"> $\small{z_{0.05} = 1.65 \le \frac{\mu - \hat{\mu}}{\hat{\sigma}/\sqrt{n}} = \frac{D \sqrt{n}}{\hat{\sigma}} \iff n \ge  
							\left(\frac{z_{0.05}\hat{\sigma}}{D}\right)^2}$</p>
							<p style="text-align: center">$\color{#cc503e}{2.73\left( \frac{\hat{\sigma}}{D} \right)^2}$</p>
						</span>
						
						<span class="fragment"><p>We barely ever have that info, and it does not acknowledge $\beta$.</p></span>
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h2>Solution II<span class="fragment" style="color: #73af48"> [easy]</span></h2>
						<span class="fragment"><p>Apply a lower $\alpha$: 0.01 or 0.005: is it better a FP or a FN?</p>
						</span>
						<span class="fragment"><p>Include effect sizes (Cohen's d, Hedges' g, <b>odds ratio</b>, <b>test statistics</b>).</p></span>
						<span class="fragment"><p>95% Confidence Interval (parametric), 2 and 98 percentiles of Walsh averages (non-parametric).</p>
						</span>
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h3>Walsh averages</h3>					
						<ul>
							<li>For $a_1, a_2, \cdots, a_n$ measurements, take all $s_{ij} = (a_i + a_j)/2\;\;i \le j$</li>
							<li>Order the list $(s_{11}, s_{12}, \cdots, )$ and take the 2 and 98 percentiles.</li>
						</ul>
						<img src="figs/V-walsh.svg" class="plain" style="text-align: : center">
					</section>
				</section>




				<section>
					<section data-background="#38A6A5">
						<aside class="notes">
							Una correlación es espúrea cuando a la hora de realizar una correlación, una variable que no debería correlar con 
							otra, por la razón que sea, lo hace. Es decir, sería el equivalente a un falso positivo de correlaciones.
						</aside>
						<h1>VI</h1>
						<h2>Spurious correlations</h2>
					</section>

					<section style="text-align: left">
						<aside class="notes">
							La correlación más común que se realiza en ciencia es la correlación de Pearson. Es una correlación útil, pero
							tiene una serie de asunciones que no suelen respetarse. EXPLICAR CADA OPCIÓN
						</aside>
						<h2>Pearson correlation</h2>
						<p>Really useful for common datasets. <span class="fragment">But it has some assumptions. The most troublesome ones are:</span></p>
						<ul>
							<span class="fragment"><li>No outliers</li></span>
							<span class="fragment"><li>Normality</li></span>
							<span class="fragment"><li>Continuity</li></span>
							<span class="fragment"><li>Linearity</li></span>
							<span class="fragment"><li>Random variables come from same distribution</li></span>
						</ul>
						<span class="fragment"><p>We will put examples of spurious correlations and their viable solutions.</p></span>
					</section>


					<section style="text-align: left">
						<h2>No outliers [Problem]</h2>
						<img src="figs/VI-correlation-pearson.svg" text-align="center" class="plain" width="100%">	
					</section>

					<section style="text-align: left">
						<h3>Solution: Spearmann correlation</h3>
						<span class="fragment"><p>Spearman correlation is calculated on the <b>rank</b> of the variables, not on their values.</p></span>
						<span class="fragment"><p>More robust to <b>outliers</b>, <b>non-linearity</b> and <b>non-normality</b>.</p></span>	
					</section>

					<section style="text-align: left">
						<h3>Solution: Spearmann correlation</h3>
						<img src="figs/VI-correlation-spearman.svg" text-align="center" class="plain" width="100%">	
					</section>

					<section style="text-align: left">
						<h3>Random variables from different distributions</h3>
						<img src="figs/VI-correlation-blobs.svg" text-align="center" class="plain" width="100%">	
					</section>


					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h2>Solution <span class="fragment" style="color: #EDAD08">[medium]</span></h2>
						<span class="fragment"><p>Assert normality and linearity. If not, try Spearmann correlation</p>
						</span>
						<span class="fragment"><p>Assert data is monomodal, or cannot be divided into categories.</p></span>
						<span class="fragment"><p>Outliers: check meaningfulness, else Spearmann</p></span>
					</section>

				</section>









				<section>
					<section data-background="#1D6996">
						<h1>VII</h1>
						<h2>Circular analysis</h2>
					</section>

					<section data-background="figs/doggo.gif" data-background-opacity=0.5>
						<h2>What is circular analysis?</h2>

						<span class="fragment"><blockquote>
							&ldquo;In statistics, circular analysis is the selection of the details of a data analysis using the data that is being analysed.&rdquo;
							<a href="https://en.wikipedia.org/wiki/Circular_analysis"> Wikipedia </a>
						</blockquote> </span>

					</section>

					<section>
						<h2>It is easy to find patterns were there are none</h2>
						<span class="fragment"><video src="figs/normal.mp4" width="49%"></video>
							<video src="figs/line.mp4" width="49%"></video>
							

						</section>
						

						<section style="text-align: left">
							<h2>Examples of circular analysis</h2>
							<ul>
								<span class="fragment"><li>Genomic analysis: apply filter until <b>our</b> set of genes appears.</li></span>
								<span class="fragment"><li>Apply filters/thresholds to points that do not "<b>fit the data</b>". </li></span>
								<span class="fragment"><li>In functional magnetic resonance imaging (fMRI) data, pre-processing is often needed, and might be applied incrementally until the analysis 'works'. (Wikipedia)</li></span>
							</ul>
						</section>

						<section style="text-align: left">
							<h2>Solution <span class="fragment" style="color: #EDAD08">[medium]</span></h2>
							<span class="fragment">Let's give a glance at machine learning (they have learned to do it well).</span>
						</section>
						

						<section>
							<aside class="notes">

							</aside>
							<img src="figs/ML-example-0.svg" class="plain" width="40%">
						</section>


						<section>
							<aside class="notes">

							</aside>
							<img src="figs/ML-example-1.svg" class="plain" width="100%">
						</section>


						<section>
							<aside class="notes">

							</aside>
							<img src="figs/ML-example-1.5.svg" class="plain" width="40%">
						</section>

						<section>
							<aside class="notes">

							</aside>
							<img src="figs/ML-example-2.svg" class="plain" width="100%">
						</section>


						<section>
							<aside class="notes">

							</aside>
							<img src="figs/ML-example-3.svg" class="plain" width="100%">
						</section>

						<section style="text-align: left">
							<h2>So... how can we apply this?</h2>
							<span class="fragment">If there are enough technical replicates / data points...</span>
							<span class="fragment">subset the data into training / test set (60/40 or 70/30).</span>

							<span class="fragment"><p></p>Consider the results as "valid" if they replicate in test set.</span>
						</section>
					</section>





					<section>
						<section data-background="#5F4690">
							<h1>VIII</h1>
							<h2>Failing to correct for <br> multiple comparisons</h2>
						</section>

						<section>
							<h2 style="text-align: left" >One statistic test at a time!</h2>
							<span class="fragment"> 
								<p style="text-align: left">We <b>must not</b> forget that one test is for a specific variable: a gene, a voxel, an experiment... </p>
								<img src="figs/p-value.svg" class="plain", style="width: 65%" align="middle"></span>
							</span>


						</section>

						<section data-transition="none-out">
							<h3 style="text-align: left">A theoretical example: gene expression</h3>
							<img src="figs/VIII-array-gray.svg" class="plain", style="width: 100%" align="middle"></span>
						</section>

						<section data-transition="none-in none-out">
							<h3 style="text-align: left">A theoretical example: gene expression</h3>
							<img src="figs/VIII-array-color.svg" class="plain", style="width: 100%" align="middle"></span>
						</section>

						<section data-transition="none-in">
							<h3 style="text-align: left">A theoretical example: gene expression</h3>
							<img src="figs/VIII-array-color-sorted.svg" class="plain", style="width: 100%" align="middle"></span>
						</section>

						<section data-background="https://media0.giphy.com/media/5NAWDQEPeFFle/giphy.gif?cid=790b761116c13fb79de8972d0ac86f4715dae09ae5942a18&rid=giphy.gif" 
						data-background-opacity="0.4">
						<h3 style="text-align: left">A practical example: a salmon in a social environment</h3>

						<a href="https://teenspecies.github.io/pdfs/NeuralCorrelates.pdf" ><p style="font-size: 80%"> Bennett CM, Baird AA, Miller MB, Wolford GL. Neural correlates of interspecies perspective taking in the post-mortem Atlantic salmon: an argument for proper multiple comparisons correction
							. <b> J Serendipitous Unexpected Results </b> 2009
						; 1: 1–5.</a>
					</section>

					<section data-transition="none-out">
						<img src="figs/salmon1.png" class="plain", style="width: 80%" align="middle"></span>
						<img src="figs/salmon2.png" class="plain", style="width: 80%" align="middle"></span>
					</section>

					<section data-transition="none-out">
						<img src="figs/salmon3.png" class="plain", style="width: 40%" align="middle"></span>
						<img src="figs/salmon4.png" class="plain", style="width: 70%; margin-top: -2%" align="middle"></span>
					</section>

					<section style="text-align: left">
						<h2>Solution <span class="fragment" style="color: #73AF48">[easy]</span></h2>
						<span class="fragment">
							<ul>
								<span class="fragment"><li>Be aware that we are multitesting.</li></span>
								<span class="fragment"><li>Apply more stringent p-values (GWAS: $\sim 10^{-8}$).</li></span>
								<span class="fragment"><li>Apply multiple test correction.
									<ul>
										<span class="fragment"><li>Bonferroni: $\alpha^* = \frac{\alpha}{N}$. <span style="color: #909090">(Maybe too stringent)</span></li>
										<li>Benjamini-Hochberg (aka FDR).</li></span>
									</ul>
								</li></span>
							</ul>
						</span>							
					</section>
				</section>



				<section>
					<section data-background="#94346E">
						<h1>IX</h1>
						<h2>Over-interpreting <br> non-significant results</h2>
					</section>

					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h3>What happens when p > 0.05?</h3>
						<span class="fragment">
							The test is negative, there is no effect. 
						</span>
						<span class="fragment">
							Well, not always. <p></p> It can imply that...
						</span>
						<ul>
							<li class="fragment">
								there is too much noise (resolutive power of technique not enough)
							</li>
							<li class="fragment">
								the dataset is too small
							</li>
						</ul>

						<span class="fragment">
							<img src="figs/IX-p.svg" class="plain">
						</span>
					</section>
					<section>
						<aside class="notes">

						</aside>
						<img src="figs/H0H1.svg" class="plain" width="40%" style="margin-bottom: -2%">
						<img src="figs/IX-p.svg" class="plain" width="80%" style="margin-bottom: -2%">
						<img src="figs/IX-p-dists.svg" class="plain" width="70%" style="margin-bottom: -2%">
					</section>
					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h2>Solution <span class="fragment" style="color: #EDAD08">[medium]</span></h2>
						<span class="fragment"><p>Assess the minimum $n$ and the statistical power before the analysis.</p>
						</span>
						<span class="fragment"><p>Assert the power is enough after analysis.</p></span>
						<span class="fragment"><p>If not sure (or not possible), do not over-interpret negative results.</p></span>
					</section>
				</section>






				<section>
					<section data-background="#6F4070">
						<h1>X</h1>
						<h2>Correlation <span style="color: #000000"> <p> does not imply </p></span> causation!</h2>
					</section>


					<section style="text-align: left">
						<aside class="notes">

						</aside>
						<h3>Finding correlations is easy</h3>
						<span class="fragment">
							<h3 style="text-align: right">explaining their cause is not.</h3>
							<p style="color: #909090; text-align: center; margin-top: 7%" > Let's see some examples.</p>
						</span>
					</section>


					<section>
						<aside class="notes">

						</aside>
						<img src="figs/X-chart1.svg" class="plain" style="width: 80%">
						<img src="figs/X-chart2.svg" class="plain" style="width: 80%; margin-top: -2%">
					</section>


					<section>
						<aside class="notes">

						</aside>
						<h3 style="text-align: left;">Nobel awards VS ... </h3>  
						<img src="figs/X-Nobel-grafica.png" class="plain" style="width: 100%">
						<a href="https://academic.oup.com/jn/article/143/6/931/4571741" > <p style="text-align: right; font-size: 0.5em" >10.3945/jn.113.174813</p>
						</a>
					</section>


					<section>
						<aside class="notes">

						</aside>
						<img src="figs/X-real-example.png" class="plain" style="width: 80%">
					</section>


					<section>
						<aside class="notes">

						</aside>
						<h3 style="text-align: left;">It all boils down to... </h3> <h3 style="text-align: right" class="fragment">confounding variables. </h3> 
						<span class="fragment"><img src="figs/X-corr-cas.svg" class="plain" style="width: 40%"></span>
						<span class="fragment"><p>Maybe more talkative / social people are perceived as more attractive, and also find better jobs? </p></span>
					</section>


					<section>
						<aside class="notes">

						</aside>
						<h3 style="text-align: left;">A more important case<span style="text-align: right" class="fragment">: smoking causes cancer</span></h3> 
						<span class="fragment" style="text-align: left;"><p style="font-size: 80%">Maybe a better diagnosis implied a higher detection rate.</p></span>
						<span class="fragment" style="text-align: left;"><p style="font-size: 80%">Maybe people genetically predispose to smoking have higher chances of lung cancer.</p></span>
						<span class="fragment" style="text-align: left;"><p style="font-size: 80%">Maybe industrial development incited easyness to tobacco, and easyness to cars $\rightarrow$ cancers are cause by pollution in general.</p></span>
						<span></span>
						<span class="fragment"><img src="figs/X-smoke-cancer.png" class="plain" style="width: 60%; margin-bottom: -3%"></span>

					</section>

					<section style="text-align: left">
						<h2>Solution <span class="fragment" style="color: #CC503E">[hard]</span></h2>
						<span class="fragment"><p>Be aware of possible underlaying confounding variables, and try to detect them</p>
						</span>
						<span class="fragment"><p>If confounding variables are found, try to adjust data.</p>
						</span>
						<span class="fragment"><p>Some methods exist to determine causality (for time series): Convergent Cross Mapping, Granger causality test.</p>
						</span>
						<span class="fragment"><p>If unsure, present conclusions as tentative</p></span>
					</section>
				</section>






				<section>
					<section data-background="#994E95">
						<h1>.</h1>
						<h2>Conclusions</h2>
					</section>

					<section>
						<aside class="notes">
							Given the amount of *only* statistical reasons a experiment might go awry, it is not uncommon to suffer from a 
							reproducibility crisis. Indeed, nature did a poll to 1576 scientists and found that 52% of them believed there was a significant reproducibility crisis, and 38% believed there was a slight crisis. We indeed can see here that, regardless of discipline, more than half of the scientists failed to replicate either their own or else's work. 
							Poor statistical analysis is one of the main causes for that outcome, but we should consider more important others as well, such as the pressure to publish or the insufficient mentoring.

						</aside>
						<h3 style="text-align: left;">Reproducibility matters</h3> 
						<img src="figs/nature-reproducibility.png" class="plain" style="width: 100%">
						<p style="text-align: right"><a href="https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970" style="font-size: 0.5em; ¡">1,500 scientists lift the lid on reproducibility</a></p>
					</section> 

					<section>
						<aside class="notes">
							Take home message:

						</aside>
						<h2>Take home message</h2> 
						<h4 class="fragment">Sometimes science is a b*tch </h4>
						<h4 class="fragment">statiticians can't do magic</h4>
						<span class="fragment"> <h4>and when they try, they end up like this</h4>
							<img src="figs/monkey.gif" class="plain"> </span>
						</section> 
					</section>

					<section>
						<section data-background="#94346E">
							<h1>Thanks!</h1>
						</section>
					</section>


				</section>
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				hash: true,
				controls: true,
				dependencies: [
				{ src: 'plugin/markdown/marked.js' },
				{ src: 'plugin/markdown/markdown.js' },
				{ src: 'plugin/highlight/highlight.js' },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/math/math.js', async: true },
				{ src: 'plugin/reveal.js-menu/menu.js' }
				],
			});
			Reveal.configure({ slideNumber: true });
		</script>
	</body>
	</html>
